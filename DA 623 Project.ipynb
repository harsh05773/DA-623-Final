{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a15390",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(r\"\"\"\n",
    "# DA623 Course Project  \n",
    "## Instruction-Guided Image Editing with InstructPix2Pix  \n",
    "**Harsh Vardhan**  \n",
    "**210104044**\n",
    "\n",
    "---\n",
    "\n",
    "### Motivation\n",
    "\n",
    "InstructPix2Pix caught my attention due to its intuitive approach to image editingâ€”using natural language instructions instead of \n",
    "complex editing tools, masks, or full image captions. I was fascinated by how large models like GPT-3 and Stable Diffusion can \n",
    "be composed to create synthetic training data, enabling a powerful model to edit real-world images based solely on instructions \n",
    "like *\"Add snow\"* or *\"Replace the sky with fireworks.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### Historical Context & Multimodal Connections\n",
    "\n",
    "Multimodal learning has evolved rapidly, especially at the intersection of language and vision. Prior works include:\n",
    "\n",
    "- **Text-guided editing with CLIP** (e.g., StyleCLIP, Text2Live)  \n",
    "- **Latent diffusion models** (Stable Diffusion, SDEdit)  \n",
    "- **Instruction tuning in LMs** (InstructGPT, RLHF)\n",
    "\n",
    "InstructPix2Pix stands out by combining these paradigms into a model that directly follows image editing instructions without \n",
    "the need for inversion or complex captioning.\n",
    "\n",
    "---\n",
    "\n",
    "### Method Overview\n",
    "\n",
    "#### Two-Stage Pipeline:\n",
    "\n",
    "**1. Dataset Generation**  \n",
    "- Fine-tune GPT-3 on ~700 caption-instruction-edited_caption triplets  \n",
    "- Generate 454K caption-instruction pairs from LAION  \n",
    "- Render image pairs using **Stable Diffusion + Prompt-to-Prompt**  \n",
    "- Filter pairs using **CLIP similarity** to ensure edit consistency  \n",
    "\n",
    "**2. Model Training**  \n",
    "- Train a latent diffusion model conditioned on image + instruction  \n",
    "- No need for inversion or per-example optimization  \n",
    "- Uses **Classifier-Free Guidance (CFG)** with dual scales:  \n",
    "    - `s_I`: fidelity to input image  \n",
    "    - `s_T`: strength of edit  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights from the Paper & GitHub\n",
    "\n",
    "- **One-pass editing**: No iterative optimization  \n",
    "- **Quick inference (~9s/image)**  \n",
    "- **Dual CFG** allows fine-grained control over edits  \n",
    "- **Generalizes well to real user instructions**  \n",
    "- **Codebase is clean** with a working Gradio demo  \n",
    "\n",
    "---\n",
    "\n",
    "### Code / Notebook (Sample Walkthrough)\n",
    "\n",
    "```bash\n",
    "# Clone and set up the environment\n",
    "git clone https://github.com/timothybrooks/instruct-pix2pix.git\n",
    "cd instruct-pix2pix\n",
    "conda env create -f environment.yaml\n",
    "conda activate ip2p\n",
    "bash scripts/download_checkpoints.sh\n",
    "\n",
    "# Run a single image edit\n",
    "python edit_cli.py \\\n",
    "    --input imgs/input.jpg \\\n",
    "    --output imgs/output.jpg \\\n",
    "    --edit \"Turn him into a cartoon character\"\n",
    "\n",
    "# Launch the interactive demo\n",
    "python edit_app.py\n",
    "\"\"\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
